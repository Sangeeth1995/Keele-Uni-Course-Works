{"cells":[{"cell_type":"markdown","metadata":{"id":"4kO7FDdRK3N_"},"source":["# Course: Application of AI, Data Science and Machine Learning\n","# Lab 9: Deep Reinforcement Learning "]},{"cell_type":"markdown","metadata":{"id":"4jvZDiG1K3OQ"},"source":["# Objective: Making a cart-pole using reinforcement leanring"]},{"cell_type":"markdown","metadata":{"id":"CFyjUne2K3OR"},"source":["# 1. Test Random Environment with OpenAI Gym"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irRGBZKrK3OS","outputId":"e894a4d3-47fd-40af-ff07-91bf09cfd09f"},"outputs":[{"data":{"text/plain":["2"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\n","#Making a cart-pole using reinforcement leanring#\n","\n","\n","\n","'''Install the folowowing dependencies if not sone already\n","\n","pip install tensorflow==2.3.0\n","pip install gym\n","pip install keras\n","pip install keras-rl2\n","'''\n","\n","#import tensorflow as tf\n","import gym \n","import random\n","\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.optimizers  import Adam\n","\n","from rl.agents import DQNAgent\n","from rl.policy import BoltzmannQPolicy\n","from rl.memory import SequentialMemory\n","\n","env = gym.make('CartPole-v0')\n","states = env.observation_space.shape[0]\n","actions = env.action_space.n\n","\n","#print('tensflow version=',tf.__version__, keras.__version__)\n","\n","actions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ep15wB2-K3OV","outputId":"af3270b7-c879-4c29-c0aa-e1da36e3029f"},"outputs":[{"name":"stdout","output_type":"stream","text":[" n_state, reward, done, info [ 0.04381702 -0.22367313  0.00673564  0.34038298] 1.0 False {}\n"," n_state, reward, done, info [ 0.03934356 -0.02864766  0.0135433   0.04983168] 1.0 False {}\n"," n_state, reward, done, info [ 0.03877061 -0.22396116  0.01453993  0.34675667] 1.0 False {}\n"," n_state, reward, done, info [ 0.03429138 -0.41928688  0.02147507  0.64398887] 1.0 False {}\n"," n_state, reward, done, info [ 0.02590565 -0.61470144  0.03435484  0.94335631] 1.0 False {}\n"," n_state, reward, done, info [ 0.01361162 -0.42005878  0.05322197  0.66166294] 1.0 False {}\n"," n_state, reward, done, info [ 0.00521044 -0.22571619  0.06645523  0.38620169] 1.0 False {}\n"," n_state, reward, done, info [ 0.00069612 -0.03159748  0.07417926  0.11518981] 1.0 False {}\n"," n_state, reward, done, info [ 6.41683588e-05  1.62387533e-01  7.64830580e-02 -1.53198807e-01] 1.0 False {}\n"," n_state, reward, done, info [ 0.00331192 -0.03374149  0.07341908  0.16259879] 1.0 False {}\n"," n_state, reward, done, info [ 0.00263709 -0.2298335   0.07667106  0.47751023] 1.0 False {}\n"," n_state, reward, done, info [-0.00195958 -0.4259495   0.08622126  0.79333921] 1.0 False {}\n"," n_state, reward, done, info [-0.01047857 -0.23211027  0.10208805  0.52897812] 1.0 False {}\n"," n_state, reward, done, info [-0.01512078 -0.42850909  0.11266761  0.85200431] 1.0 False {}\n"," n_state, reward, done, info [-0.02369096 -0.62497189  0.1297077   1.17788431] 1.0 False {}\n"," n_state, reward, done, info [-0.0361904  -0.43175079  0.15326538  0.92851455] 1.0 False {}\n"," n_state, reward, done, info [-0.04482541 -0.23899295  0.17183567  0.68765091] 1.0 False {}\n"," n_state, reward, done, info [-0.04960527 -0.43603063  0.18558869  1.02912607] 1.0 False {}\n"," n_state, reward, done, info [-0.05832588 -0.63307192  0.20617121  1.37386295] 1.0 False {}\n"," n_state, reward, done, info [-0.07098732 -0.44103558  0.23364847  1.15209471] 1.0 True {}\n","Episode:1 Score:20.0\n"," n_state, reward, done, info [ 0.03264445 -0.20790926 -0.04731538  0.29491227] 1.0 False {}\n"," n_state, reward, done, info [ 0.02848626 -0.40232584 -0.04141714  0.57230524] 1.0 False {}\n"," n_state, reward, done, info [ 0.02043975 -0.20664836 -0.02997103  0.26686757] 1.0 False {}\n"," n_state, reward, done, info [ 0.01630678 -0.01111177 -0.02463368 -0.03511576] 1.0 False {}\n"," n_state, reward, done, info [ 0.01608454 -0.20587196 -0.025336    0.24969433] 1.0 False {}\n"," n_state, reward, done, info [ 0.0119671  -0.01039753 -0.02034211 -0.0508712 ] 1.0 False {}\n"," n_state, reward, done, info [ 0.01175915 -0.20522198 -0.02135953  0.23532485] 1.0 False {}\n"," n_state, reward, done, info [ 0.00765471 -0.40003235 -0.01665304  0.52119452] 1.0 False {}\n"," n_state, reward, done, info [-0.00034593 -0.20467999 -0.00622915  0.22331084] 1.0 False {}\n"," n_state, reward, done, info [-0.00443953 -0.39971236 -0.00176293  0.51402236] 1.0 False {}\n"," n_state, reward, done, info [-0.01243378 -0.59480944  0.00851752  0.80614922] 1.0 False {}\n"," n_state, reward, done, info [-0.02432997 -0.39980527  0.0246405   0.5161577 ] 1.0 False {}\n"," n_state, reward, done, info [-0.03232607 -0.59526537  0.03496366  0.81650243] 1.0 False {}\n"," n_state, reward, done, info [-0.04423138 -0.40063911  0.0512937   0.53501868] 1.0 False {}\n"," n_state, reward, done, info [-0.05224416 -0.20627456  0.06199408  0.25892905] 1.0 False {}\n"," n_state, reward, done, info [-0.05636965 -0.40222424  0.06717266  0.57050351] 1.0 False {}\n"," n_state, reward, done, info [-0.06441414 -0.20810542  0.07858273  0.29971588] 1.0 False {}\n"," n_state, reward, done, info [-0.06857625 -0.01418639  0.08457705  0.03281455] 1.0 False {}\n"," n_state, reward, done, info [-0.06885998 -0.21041298  0.08523334  0.35093832] 1.0 False {}\n"," n_state, reward, done, info [-0.07306824 -0.4066371   0.0922521   0.66923429] 1.0 False {}\n"," n_state, reward, done, info [-0.08120098 -0.21291073  0.10563679  0.40696405] 1.0 False {}\n"," n_state, reward, done, info [-0.08545919 -0.40935958  0.11377607  0.73099418] 1.0 False {}\n"," n_state, reward, done, info [-0.09364638 -0.60585469  0.12839595  1.05720924] 1.0 False {}\n"," n_state, reward, done, info [-0.10576348 -0.80242243  0.14954014  1.38727856] 1.0 False {}\n"," n_state, reward, done, info [-0.12181193 -0.99905809  0.17728571  1.7227414 ] 1.0 False {}\n"," n_state, reward, done, info [-0.14179309 -1.19571145  0.21174054  2.06494733] 1.0 True {}\n","Episode:2 Score:26.0\n"," n_state, reward, done, info [-0.00651251 -0.15566561  0.03062924  0.27110152] 1.0 False {}\n"," n_state, reward, done, info [-0.00962582 -0.35121094  0.03605127  0.57328552] 1.0 False {}\n"," n_state, reward, done, info [-0.01665004 -0.54681932  0.04751698  0.87710422] 1.0 False {}\n"," n_state, reward, done, info [-0.02758642 -0.74255375  0.06505907  1.1843392 ] 1.0 False {}\n"," n_state, reward, done, info [-0.0424375  -0.54833331  0.08874585  0.91273875] 1.0 False {}\n"," n_state, reward, done, info [-0.05340417 -0.35451679  0.10700063  0.64921513] 1.0 False {}\n"," n_state, reward, done, info [-0.0604945  -0.55095375  0.11998493  0.97358359] 1.0 False {}\n"," n_state, reward, done, info [-0.07151358 -0.35762808  0.1394566   0.72087096] 1.0 False {}\n"," n_state, reward, done, info [-0.07866614 -0.55437557  0.15387402  1.05399453] 1.0 False {}\n"," n_state, reward, done, info [-0.08975365 -0.36159098  0.17495391  0.81329501] 1.0 False {}\n"," n_state, reward, done, info [-0.09698547 -0.16924133  0.19121981  0.58034944] 1.0 False {}\n"," n_state, reward, done, info [-0.1003703   0.02275959  0.2028268   0.35347406] 1.0 False {}\n"," n_state, reward, done, info [-0.0999151   0.21450707  0.20989628  0.13097183] 1.0 True {}\n","Episode:3 Score:13.0\n"," n_state, reward, done, info [-0.0135604   0.16945494  0.01756691 -0.25092479] 1.0 False {}\n"," n_state, reward, done, info [-0.0101713  -0.0259134   0.01254841  0.04724692] 1.0 False {}\n"," n_state, reward, done, info [-0.01068957 -0.22121302  0.01349335  0.34386242] 1.0 False {}\n"," n_state, reward, done, info [-0.01511383 -0.0262856   0.0203706   0.05546483] 1.0 False {}\n"," n_state, reward, done, info [-0.01563954 -0.22169362  0.0214799   0.35450459] 1.0 False {}\n"," n_state, reward, done, info [-0.02007341 -0.02688357  0.02856999  0.06867153] 1.0 False {}\n"," n_state, reward, done, info [-0.02061109  0.16781738  0.02994342 -0.21486227] 1.0 False {}\n"," n_state, reward, done, info [-0.01725474 -0.02771958  0.02564617  0.08711373] 1.0 False {}\n"," n_state, reward, done, info [-0.01780913 -0.22319958  0.02738845  0.38777645] 1.0 False {}\n"," n_state, reward, done, info [-0.02227312 -0.41869938  0.03514398  0.68896737] 1.0 False {}\n"," n_state, reward, done, info [-0.03064711 -0.22408233  0.04892332  0.40755227] 1.0 False {}\n"," n_state, reward, done, info [-0.03512876 -0.41986261  0.05707437  0.71524903] 1.0 False {}\n"," n_state, reward, done, info [-0.04352601 -0.2255752   0.07137935  0.44106321] 1.0 False {}\n"," n_state, reward, done, info [-0.04803751 -0.42163086  0.08020061  0.75536555] 1.0 False {}\n"," n_state, reward, done, info [-0.05647013 -0.61776138  0.09530792  1.07216938] 1.0 False {}\n"," n_state, reward, done, info [-0.06882536 -0.81400515  0.11675131  1.39317724] 1.0 False {}\n"," n_state, reward, done, info [-0.08510546 -0.62051394  0.14461486  1.13916322] 1.0 False {}\n"," n_state, reward, done, info [-0.09751574 -0.42754797  0.16739812  0.89510441] 1.0 False {}\n"," n_state, reward, done, info [-0.1060667  -0.23504295  0.18530021  0.65936876] 1.0 False {}\n"," n_state, reward, done, info [-0.11076756 -0.04291711  0.19848758  0.43028054] 1.0 False {}\n"," n_state, reward, done, info [-0.1116259  -0.24021454  0.2070932   0.77838899] 1.0 False {}\n"," n_state, reward, done, info [-0.11643019 -0.4374905   0.22266098  1.12843119] 1.0 True {}\n","Episode:4 Score:22.0\n"," n_state, reward, done, info [ 0.02696298 -0.16422212  0.00329129  0.33544501] 1.0 False {}\n"," n_state, reward, done, info [0.02367854 0.03085284 0.01000019 0.0438018 ] 1.0 False {}\n"," n_state, reward, done, info [ 0.0242956  -0.16441107  0.01087622  0.33962303] 1.0 False {}\n"," n_state, reward, done, info [0.02100738 0.03055445 0.01766868 0.05038959] 1.0 False {}\n"," n_state, reward, done, info [ 0.02161846  0.22541865  0.01867647 -0.23666676] 1.0 False {}\n"," n_state, reward, done, info [ 0.02612684  0.42026886  0.01394314 -0.52340055] 1.0 False {}\n"," n_state, reward, done, info [ 0.03453221  0.22495348  0.00347513 -0.2263568 ] 1.0 False {}\n"," n_state, reward, done, info [ 0.03903128  0.02978203 -0.00105201  0.06742029] 1.0 False {}\n"," n_state, reward, done, info [ 3.96269248e-02 -1.65324820e-01  2.96398593e-04  3.59771114e-01] 1.0 False {}\n"," n_state, reward, done, info [ 0.03632043 -0.36045098  0.00749182  0.65254749] 1.0 False {}\n"," n_state, reward, done, info [ 0.02911141 -0.55567646  0.02054277  0.94758006] 1.0 False {}\n"," n_state, reward, done, info [ 0.01799788 -0.75106891  0.03949437  1.24664605] 1.0 False {}\n"," n_state, reward, done, info [ 0.0029765  -0.55647511  0.06442729  0.96659129] 1.0 False {}\n"," n_state, reward, done, info [-0.008153   -0.3622748   0.08375912  0.69482372] 1.0 False {}\n"," n_state, reward, done, info [-0.0153985  -0.16840835  0.09765559  0.42963991] 1.0 False {}\n"," n_state, reward, done, info [-0.01876666 -0.3647678   0.10624839  0.75144087] 1.0 False {}\n"," n_state, reward, done, info [-0.02606202 -0.56118201  0.12127721  1.0755791 ] 1.0 False {}\n"]},{"name":"stdout","output_type":"stream","text":[" n_state, reward, done, info [-0.03728566 -0.75767927  0.14278879  1.40372821] 1.0 False {}\n"," n_state, reward, done, info [-0.05243925 -0.56458994  0.17086335  1.15887923] 1.0 False {}\n"," n_state, reward, done, info [-0.06373104 -0.76147562  0.19404094  1.49989703] 1.0 False {}\n"," n_state, reward, done, info [-0.07896056 -0.95835223  0.22403888  1.8463605 ] 1.0 True {}\n","Episode:5 Score:21.0\n"]}],"source":["episodes = 5\n","for episode in range(1, episodes+1):\n","    state = env.reset()\n","    done = False\n","    score = 0 \n","    \n","    while not done:\n","        env.render()\n","        action = random.choice([0,1])\n","        n_state, reward, done, info = env.step(action)\n","        print(' n_state, reward, done, info' , n_state, reward, done, info)\n","        score+=reward\n","    print('Episode:{} Score:{}'.format(episode, score))"]},{"cell_type":"markdown","metadata":{"id":"iLYjZwSEK3OX"},"source":["#  2 Create a Deep Learning Model with Keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7q0tkNuK3OY"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxTkvUAkK3OZ","outputId":"28bc8f6d-6ebc-4372-b064-f4c9b6c3487b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten (Flatten)            (None, 4)                 0         \n","_________________________________________________________________\n","dense (Dense)                (None, 24)                120       \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 24)                600       \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 50        \n","=================================================================\n","Total params: 770\n","Trainable params: 770\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["def build_model(states, actions):\n","    model = Sequential()\n","    model.add(Flatten(input_shape=(1,states)))\n","    model.add(Dense(24, activation='relu'))\n","    model.add(Dense(24, activation='relu'))\n","    model.add(Dense(actions, activation='linear'))\n","    return model\n","\n","model = build_model(states, actions)\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"2ntpKYFeK3Oa"},"source":["# 3. Build Agent with Keras-RL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGseyZGyK3Ob","outputId":"f3ab24ef-22d6-4283-b9c3-086fc45d8b99"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training for 20000 steps ...\n","Interval 1 (0 steps performed)\n","WARNING:tensorflow:From C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","    1/10000 [..............................] - ETA: 11:31 - reward: 1.0000"]},{"name":"stderr","output_type":"stream","text":["C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\rl\\memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"]},{"name":"stdout","output_type":"stream","text":["10000/10000 [==============================] - 152s 15ms/step - reward: 1.0000\n","92 episodes - episode_reward: 107.054 [11.000, 200.000] - loss: 4.580 - mae: 19.897 - mean_q: 40.140\n","\n","Interval 2 (10000 steps performed)\n","10000/10000 [==============================] - 172s 17ms/step - reward: 1.0000\n","done, took 324.306 seconds\n","Testing for 100 episodes ...\n","Episode 1: reward: 200.000, steps: 200\n","Episode 2: reward: 200.000, steps: 200\n","Episode 3: reward: 200.000, steps: 200\n","Episode 4: reward: 200.000, steps: 200\n","Episode 5: reward: 200.000, steps: 200\n","Episode 6: reward: 200.000, steps: 200\n","Episode 7: reward: 200.000, steps: 200\n","Episode 8: reward: 200.000, steps: 200\n","Episode 9: reward: 200.000, steps: 200\n","Episode 10: reward: 200.000, steps: 200\n","Episode 11: reward: 200.000, steps: 200\n","Episode 12: reward: 200.000, steps: 200\n","Episode 13: reward: 200.000, steps: 200\n","Episode 14: reward: 200.000, steps: 200\n","Episode 15: reward: 200.000, steps: 200\n","Episode 16: reward: 200.000, steps: 200\n","Episode 17: reward: 200.000, steps: 200\n","Episode 18: reward: 200.000, steps: 200\n","Episode 19: reward: 200.000, steps: 200\n","Episode 20: reward: 200.000, steps: 200\n","Episode 21: reward: 200.000, steps: 200\n","Episode 22: reward: 200.000, steps: 200\n","Episode 23: reward: 200.000, steps: 200\n","Episode 24: reward: 200.000, steps: 200\n","Episode 25: reward: 200.000, steps: 200\n","Episode 26: reward: 200.000, steps: 200\n","Episode 27: reward: 200.000, steps: 200\n","Episode 28: reward: 200.000, steps: 200\n","Episode 29: reward: 200.000, steps: 200\n","Episode 30: reward: 200.000, steps: 200\n","Episode 31: reward: 200.000, steps: 200\n","Episode 32: reward: 200.000, steps: 200\n","Episode 33: reward: 200.000, steps: 200\n","Episode 34: reward: 200.000, steps: 200\n","Episode 35: reward: 200.000, steps: 200\n","Episode 36: reward: 200.000, steps: 200\n","Episode 37: reward: 200.000, steps: 200\n","Episode 38: reward: 200.000, steps: 200\n","Episode 39: reward: 200.000, steps: 200\n","Episode 40: reward: 200.000, steps: 200\n","Episode 41: reward: 200.000, steps: 200\n","Episode 42: reward: 200.000, steps: 200\n","Episode 43: reward: 200.000, steps: 200\n","Episode 44: reward: 200.000, steps: 200\n","Episode 45: reward: 200.000, steps: 200\n","Episode 46: reward: 200.000, steps: 200\n","Episode 47: reward: 200.000, steps: 200\n","Episode 48: reward: 200.000, steps: 200\n","Episode 49: reward: 200.000, steps: 200\n","Episode 50: reward: 200.000, steps: 200\n","Episode 51: reward: 200.000, steps: 200\n","Episode 52: reward: 200.000, steps: 200\n","Episode 53: reward: 200.000, steps: 200\n","Episode 54: reward: 200.000, steps: 200\n","Episode 55: reward: 200.000, steps: 200\n","Episode 56: reward: 200.000, steps: 200\n","Episode 57: reward: 200.000, steps: 200\n","Episode 58: reward: 200.000, steps: 200\n","Episode 59: reward: 200.000, steps: 200\n","Episode 60: reward: 200.000, steps: 200\n","Episode 61: reward: 200.000, steps: 200\n","Episode 62: reward: 200.000, steps: 200\n","Episode 63: reward: 200.000, steps: 200\n","Episode 64: reward: 200.000, steps: 200\n","Episode 65: reward: 200.000, steps: 200\n","Episode 66: reward: 200.000, steps: 200\n","Episode 67: reward: 200.000, steps: 200\n","Episode 68: reward: 200.000, steps: 200\n","Episode 69: reward: 200.000, steps: 200\n","Episode 70: reward: 200.000, steps: 200\n","Episode 71: reward: 200.000, steps: 200\n","Episode 72: reward: 200.000, steps: 200\n","Episode 73: reward: 200.000, steps: 200\n","Episode 74: reward: 200.000, steps: 200\n","Episode 75: reward: 200.000, steps: 200\n","Episode 76: reward: 200.000, steps: 200\n","Episode 77: reward: 200.000, steps: 200\n","Episode 78: reward: 200.000, steps: 200\n","Episode 79: reward: 200.000, steps: 200\n","Episode 80: reward: 200.000, steps: 200\n","Episode 81: reward: 200.000, steps: 200\n","Episode 82: reward: 200.000, steps: 200\n","Episode 83: reward: 200.000, steps: 200\n","Episode 84: reward: 200.000, steps: 200\n","Episode 85: reward: 200.000, steps: 200\n","Episode 86: reward: 200.000, steps: 200\n","Episode 87: reward: 200.000, steps: 200\n","Episode 88: reward: 200.000, steps: 200\n","Episode 89: reward: 200.000, steps: 200\n","Episode 90: reward: 200.000, steps: 200\n","Episode 91: reward: 200.000, steps: 200\n","Episode 92: reward: 200.000, steps: 200\n","Episode 93: reward: 200.000, steps: 200\n","Episode 94: reward: 200.000, steps: 200\n","Episode 95: reward: 200.000, steps: 200\n","Episode 96: reward: 200.000, steps: 200\n","Episode 97: reward: 200.000, steps: 200\n","Episode 98: reward: 200.000, steps: 200\n","Episode 99: reward: 200.000, steps: 200\n","Episode 100: reward: 200.000, steps: 200\n","200.0\n"]}],"source":["\n","\n","\n","def build_agent(model, actions):\n","    policy = BoltzmannQPolicy()\n","    memory = SequentialMemory(limit=50000, window_length=1)\n","    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n","                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n","    return dqn\n","\n","#1e-2 = 10^-2\n","\n","\n","dqn = build_agent(model, actions)\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","dqn.fit(env, nb_steps=20000, visualize=False, verbose=1)\n","\n","\n","\n","scores = dqn.test(env, nb_episodes=100, visualize=False)\n","print(np.mean(scores.history['episode_reward']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"me9V2y8XK3Od","outputId":"69711979-2975-430e-dd3f-8d3591dc5103"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing for 100 episodes ...\n","Episode 1: reward: 200.000, steps: 200\n","Episode 2: reward: 200.000, steps: 200\n","Episode 3: reward: 200.000, steps: 200\n","Episode 4: reward: 200.000, steps: 200\n","Episode 5: reward: 200.000, steps: 200\n","Episode 6: reward: 200.000, steps: 200\n","Episode 7: reward: 200.000, steps: 200\n","Episode 8: reward: 200.000, steps: 200\n","Episode 9: reward: 200.000, steps: 200\n","Episode 10: reward: 200.000, steps: 200\n","Episode 11: reward: 200.000, steps: 200\n","Episode 12: reward: 200.000, steps: 200\n","Episode 13: reward: 200.000, steps: 200\n","Episode 14: reward: 200.000, steps: 200\n","Episode 15: reward: 200.000, steps: 200\n","Episode 16: reward: 200.000, steps: 200\n","Episode 17: reward: 200.000, steps: 200\n","Episode 18: reward: 200.000, steps: 200\n","Episode 19: reward: 200.000, steps: 200\n","Episode 20: reward: 200.000, steps: 200\n","Episode 21: reward: 200.000, steps: 200\n","Episode 22: reward: 200.000, steps: 200\n","Episode 23: reward: 200.000, steps: 200\n","Episode 24: reward: 200.000, steps: 200\n","Episode 25: reward: 200.000, steps: 200\n","Episode 26: reward: 200.000, steps: 200\n","Episode 27: reward: 200.000, steps: 200\n","Episode 28: reward: 200.000, steps: 200\n","Episode 29: reward: 200.000, steps: 200\n","Episode 30: reward: 200.000, steps: 200\n","Episode 31: reward: 200.000, steps: 200\n","Episode 32: reward: 200.000, steps: 200\n","Episode 33: reward: 200.000, steps: 200\n","Episode 34: reward: 200.000, steps: 200\n","Episode 35: reward: 200.000, steps: 200\n","Episode 36: reward: 200.000, steps: 200\n","Episode 37: reward: 200.000, steps: 200\n","Episode 38: reward: 200.000, steps: 200\n","Episode 39: reward: 200.000, steps: 200\n","Episode 40: reward: 200.000, steps: 200\n","Episode 41: reward: 200.000, steps: 200\n","Episode 42: reward: 200.000, steps: 200\n","Episode 43: reward: 200.000, steps: 200\n","Episode 44: reward: 200.000, steps: 200\n","Episode 45: reward: 200.000, steps: 200\n","Episode 46: reward: 200.000, steps: 200\n","Episode 47: reward: 200.000, steps: 200\n","Episode 48: reward: 200.000, steps: 200\n","Episode 49: reward: 200.000, steps: 200\n","Episode 50: reward: 200.000, steps: 200\n","Episode 51: reward: 200.000, steps: 200\n","Episode 52: reward: 200.000, steps: 200\n","Episode 53: reward: 200.000, steps: 200\n","Episode 54: reward: 200.000, steps: 200\n","Episode 55: reward: 200.000, steps: 200\n","Episode 56: reward: 200.000, steps: 200\n","Episode 57: reward: 200.000, steps: 200\n","Episode 58: reward: 200.000, steps: 200\n","Episode 59: reward: 200.000, steps: 200\n","Episode 60: reward: 200.000, steps: 200\n","Episode 61: reward: 200.000, steps: 200\n","Episode 62: reward: 200.000, steps: 200\n","Episode 63: reward: 200.000, steps: 200\n","Episode 64: reward: 200.000, steps: 200\n","Episode 65: reward: 200.000, steps: 200\n","Episode 66: reward: 200.000, steps: 200\n","Episode 67: reward: 200.000, steps: 200\n","Episode 68: reward: 200.000, steps: 200\n","Episode 69: reward: 200.000, steps: 200\n","Episode 70: reward: 200.000, steps: 200\n","Episode 71: reward: 200.000, steps: 200\n","Episode 72: reward: 200.000, steps: 200\n","Episode 73: reward: 200.000, steps: 200\n","Episode 74: reward: 200.000, steps: 200\n","Episode 75: reward: 200.000, steps: 200\n","Episode 76: reward: 200.000, steps: 200\n","Episode 77: reward: 200.000, steps: 200\n","Episode 78: reward: 200.000, steps: 200\n","Episode 79: reward: 200.000, steps: 200\n","Episode 80: reward: 200.000, steps: 200\n","Episode 81: reward: 200.000, steps: 200\n","Episode 82: reward: 200.000, steps: 200\n","Episode 83: reward: 200.000, steps: 200\n","Episode 84: reward: 200.000, steps: 200\n","Episode 85: reward: 200.000, steps: 200\n","Episode 86: reward: 200.000, steps: 200\n","Episode 87: reward: 200.000, steps: 200\n","Episode 88: reward: 200.000, steps: 200\n","Episode 89: reward: 200.000, steps: 200\n","Episode 90: reward: 200.000, steps: 200\n","Episode 91: reward: 200.000, steps: 200\n","Episode 92: reward: 200.000, steps: 200\n","Episode 93: reward: 200.000, steps: 200\n","Episode 94: reward: 200.000, steps: 200\n","Episode 95: reward: 200.000, steps: 200\n","Episode 96: reward: 200.000, steps: 200\n","Episode 97: reward: 200.000, steps: 200\n","Episode 98: reward: 200.000, steps: 200\n","Episode 99: reward: 200.000, steps: 200\n","Episode 100: reward: 200.000, steps: 200\n","200.0\n"]}],"source":["scores = dqn.test(env, nb_episodes=100, visualize=False)\n","print(np.mean(scores.history['episode_reward']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aereK7aQK3Oe","outputId":"c0b5f73e-5891-4d71-f6cc-72d6bf0d905d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing for 15 episodes ...\n","Episode 1: reward: 200.000, steps: 200\n","Episode 2: reward: 200.000, steps: 200\n","Episode 3: reward: 200.000, steps: 200\n","Episode 4: reward: 200.000, steps: 200\n","Episode 5: reward: 200.000, steps: 200\n","Episode 6: reward: 200.000, steps: 200\n","Episode 7: reward: 200.000, steps: 200\n","Episode 8: reward: 200.000, steps: 200\n","Episode 9: reward: 200.000, steps: 200\n","Episode 10: reward: 200.000, steps: 200\n","Episode 11: reward: 200.000, steps: 200\n","Episode 12: reward: 200.000, steps: 200\n","Episode 13: reward: 200.000, steps: 200\n","Episode 14: reward: 200.000, steps: 200\n","Episode 15: reward: 200.000, steps: 200\n"]}],"source":["_ = dqn.test(env, nb_episodes=15, visualize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wH1bX4uYK3Of"},"outputs":[],"source":["#dqn.save_weights('dqn_weights.h5f', overwrite=True)"]},{"cell_type":"markdown","metadata":{"id":"mYyzZTqJK3Of"},"source":[" # Credit:https://github.com/nicknochnack/TensorflowKeras-ReinforcementLearning/blob/master/Deep%20Reinforcement%20Learning.ipynb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G94lB04fK3Og"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Lab-9-Deep-Reinforcement-Learning_lecturor.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}