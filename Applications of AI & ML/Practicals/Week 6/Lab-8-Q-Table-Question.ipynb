{"cells":[{"cell_type":"markdown","metadata":{"id":"OvvEauUpJzv4"},"source":["# Course: Application of AI, Data Science and Machine Learning\n","# Lab 8: Reinforcement Learning (Implementing Q-Table)"]},{"cell_type":"markdown","metadata":{"id":"iTBc5F6PJzwV"},"source":["### This lab will need Open AI Gym. \n","\n","Gym has a ton of environments ranging from simple text based games to Atari games like Breakout and Space Invaders. The library is intuitive to use and simple to install. \n","\n","Just run pip install gym, and you're good to go! \n","\n","The link to Gym's installation instructions, requirements, and documentation is: https://gym.openai.com/docs/\n","\n","Go ahead and get that installed now because we'll need it in just a moment. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-AdPO0uJzwg","outputId":"1b558f00-606c-447b-9ba5-ade0f01dabf4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gym\n","  Downloading gym-0.23.0.tar.gz (624 kB)\n","\u001b[K     |████████████████████████████████| 624 kB 541 kB/s eta 0:00:01\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n","\u001b[?25hCollecting gym-notices>=0.0.4\n","  Downloading gym_notices-0.0.6-py3-none-any.whl (2.7 kB)\n","Requirement already satisfied: importlib-metadata>=4.10.0 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from gym) (4.11.1)\n","Requirement already satisfied: numpy>=1.18.0 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.20.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.6.0)\n","Requirement already satisfied: zipp>=0.5 in /Users/macbook/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.10.0->gym) (3.4.1)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697659 sha256=6e5d4270845c56cea2db36e2d8dbe513c44f629d64a82436056b3ccabe990c9f\n","  Stored in directory: /Users/macbook/Library/Caches/pip/wheels/e7/2f/ab/68bf956c5dde73c1856d981e54292cf58385fb60bca10b7acd\n","Successfully built gym\n","Installing collected packages: gym-notices, gym\n","Successfully installed gym-0.23.0 gym-notices-0.0.6\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWDn3TMeJzwj"},"outputs":[],"source":["import gym"]},{"cell_type":"markdown","metadata":{"id":"xm4IsrxjJzwk"},"source":["### Frozen Lake problem description: \n","Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following:\n","\n","![image.png](attachment:image.png)\n","This grid is our environment where S is the agent's starting point, and it's safe. F represents the frozen surface and is also safe. H represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that's not good. Finally, G represents the goal, which is the space on the grid where the prized frisbee is located.\n","\n","The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise.\n","\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"ju9PvG10Jzwm"},"source":["### Step 1: Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vos5_HP2Jzwo"},"outputs":[],"source":["# Note this code will run inside tensorflow environement\n","import numpy as np\n","import gym\n","import random\n","import time\n","from IPython.display import clear_output"]},{"cell_type":"markdown","metadata":{"id":"NsJZuoDNJzwp"},"source":["#### Step 2: Set up the Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVyaMaBiJzwq"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzAtcGHmJzwr","outputId":"af59223e-7fde-4f10-c5db-f6a9b76ec82d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pygame\n","  Downloading pygame-2.1.2-cp38-cp38-macosx_10_9_x86_64.whl (8.9 MB)\n","\u001b[K     |████████████████████████████████| 8.9 MB 3.0 MB/s eta 0:00:01     |█████▌                          | 1.5 MB 1.6 MB/s eta 0:00:05\n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.1.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pygame"]},{"cell_type":"markdown","metadata":{"id":"q_yOb8KkJzw3"},"source":["### Step 3: Create the Q-Table\n","\n","We're now going to construct our Q-table, and initialize all the Q-values to zero for each state-action pair.\n","\n","Remember, the number of rows in the table is equivalent to the size of the state space in the environment, and the number of columns is equivalent to the size of the action space. We can get this information using using env.observation_space.n and env.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOFRq8czJzw5"},"outputs":[],"source":["action_size = env.action_space.n\n","state_size = env.observation_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtlxjlWVJzw6","outputId":"28714929-2f88-4f3c-d165-1d2aea93dfd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["qtable = np.zeros((state_size, action_size))\n","print(qtable)"]},{"cell_type":"markdown","metadata":{"id":"Arw8Eli0Jzw7"},"source":["#### Step 4: Initializing Q-Learning Parameters\n","\n","First, with num_episodes, we define the total number of episodes we want the agent to play during training. Then, with max_steps_per_episode, we define a maximum number of steps that our agent is allowed to take within a single episode. So, if by the one-hundredth step, the agent hasn't reached the frisbee or fallen through a hole, then the episode will terminate with the agent receiving zero points.\n","\n","Next, we set our learning_rate, which was mathematically shown using the symbol  in the previous post. Then, we also set our discount_rate, as well, which was represented with the symbol  previously.\n","\n","Initialize our exploration_rate to 1 and setting the max_exploration_rate to 1 and a min_exploration_rate to 0.01. The max and min are just bounds to how large or small our exploration rate can be. Remember, the exploration rate was represented with the symbol  when we discussed it previously.\n","\n","Lastly, we set the exploration_decay_rate to 0.01 to determine the rate at which the exploration_rate will decay."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"845cly1TJzw8"},"outputs":[],"source":["total_episodes = 15000        # Total episodes\n","learning_rate = 0.8           # Learning rate\n","max_steps = 99                # Max steps per episode\n","gamma = 0.95                  # Discounting rate\n","\n","# Exploration parameters\n","epsilon = 1.0                 # Exploration rate\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability \n","decay_rate = 0.005            # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{"id":"B_s3yXAzJzw9"},"source":["#### Step 5: Coding The Q-Learning Algorithm Training Loop\n","\n","First, we create this list to hold all of the rewards we'll get from each episode. This will be so we can see how our game score changes over time. We'll discuss this more in a bit.\n","\n","rewards_all_episodes = []\n","\n","#### Q-learning algorithm\n","for episode in range(num_episodes):\n","    # initialize new episode params\n","\n","    for step in range(max_steps_per_episode): \n","        # Exploration-exploitation trade-off\n","        # Take new action\n","        # Update Q-table\n","        # Set new state\n","        # Add new reward        \n","\n","    # Exploration rate decay  \n","    # Add current episode reward to total rewards list  (step is optional, not implemented in the solution provided in this lab)\n","\n","\n","#print Q-table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AohsXNbeJzw9","outputId":"d588281a-5580-47cf-b13a-331510d55a20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Score over time: 0.4744\n","[[6.15910507e-02 6.08926641e-02 8.07013382e-02 4.34393078e-02]\n"," [3.66897341e-03 8.03684328e-03 8.84262341e-04 6.71879405e-02]\n"," [1.58279246e-03 1.17460596e-02 1.90034468e-02 5.44301622e-02]\n"," [8.69807421e-03 4.14833519e-03 4.62091832e-03 2.84615198e-02]\n"," [9.96132507e-02 6.48231358e-03 7.12031536e-03 2.26356728e-03]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [6.25127488e-05 4.92529916e-11 2.19829023e-02 4.06091784e-05]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [2.75823010e-03 2.14211208e-02 7.53239464e-02 1.04658560e-01]\n"," [1.48605100e-02 6.61831596e-02 5.42979605e-01 7.26192956e-02]\n"," [1.45864894e-02 7.42437824e-04 5.32588139e-03 3.16073972e-03]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [2.34225383e-03 6.87463745e-02 8.75693769e-01 1.87911767e-02]\n"," [3.54744381e-01 9.91700257e-01 8.29983613e-02 3.48101319e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"]}],"source":["# List of rewards\n","rewards = []\n","\n","# 2 For life or until learning is stopped\n","for episode in range(total_episodes):\n","    # Reset the environment\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards = 0\n","    \n","    for step in range(max_steps):\n","        # 3. Choose an action a in the current world state (s)\n","        ## First we randomize a number\n","        exp_exp_tradeoff = random.uniform(0, 1)\n","        \n","        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n","        if exp_exp_tradeoff > epsilon:\n","            action = np.argmax(qtable[state,:])\n","\n","        # Else doing a random choice --> exploration\n","        else:\n","            action = env.action_space.sample()\n","\n","        # Take the action (a) and observe the outcome state(s') and reward (r)\n","        new_state, reward, done, info = env.step(action)\n","\n","        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","        # qtable[new_state,:] : all the actions we can take from new state\n","        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","        \n","        total_rewards += reward\n","        \n","        # Our new state is state\n","        state = new_state\n","        \n","        # If done (if we're dead) : finish episode\n","        if done == True: \n","            break\n","        \n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n","    rewards.append(total_rewards)\n","\n","print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n","print(qtable)"]},{"cell_type":"markdown","metadata":{"id":"rXrDyZOYJzxD"},"source":["#### Step 6: The Code To Watch The Agent Play The Game\n","\n","#### Watch our agent play Frozen Lake by playing the best action \n","#### from each state according to the Q-table\n","\n","for episode in range(3):\n","    # initialize new episode params\n","\n","    for step in range(max_steps_per_episode):        \n","        # Show current state of environment on screen\n","        # Choose action with highest Q-value for current state       \n","        # Take new action\n","\n","        if done:\n","            if reward == 1:\n","                # Agent reached the goal and won episode\n","            else:\n","                # Agent stepped in a hole and lost episode            \n","\n","        # Set new state\n","\n","env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mc8UVS71JzxE","outputId":"ebaaf029-3beb-457d-d173-f23af79aaaaa"},"outputs":[{"name":"stdout","output_type":"stream","text":["****************************************************\n","EPISODE  0\n","Number of steps 71\n","****************************************************\n","EPISODE  1\n","Number of steps 33\n","****************************************************\n","EPISODE  2\n","Number of steps 58\n","****************************************************\n","EPISODE  3\n","Number of steps 9\n","****************************************************\n","EPISODE  4\n"]}],"source":["env.reset()\n","\n","for episode in range(5):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"EPISODE \", episode)\n","\n","    for step in range(max_steps):\n","        \n","        # Take the action (index) that have the maximum expected future reward given that state\n","        action = np.argmax(qtable[state,:])\n","        \n","        new_state, reward, done, info = env.step(action)\n","        \n","        if done:\n","            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n","            env.render()\n","            \n","            # We print the number of step it took.\n","            print(\"Number of steps\", step)\n","            break\n","        state = new_state\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"btur7V_WJzxF"},"source":["## Code Credit:\n","https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb\n","\n","\n","## Useful videos related to code. You can refer videos 8, 9, 10:\n","Link: https://deeplizard.com/learn/playlist/PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dm3bECetJzxG"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Lab-8-Q-Table-Question.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}